# -*- coding: utf-8 -*-
"""TigerSafe_EDA_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1whdBPYZNzQDVW5gm6mqi4vlb6oNiAtP1

# TigerSafe: Exploratory Data Analysis, Visualization & Feature Engineering
## Safety-Aware Routing for Mizzou Students — Columbia, MO

**Project Goal:** Build a safety-aware routing app that combines crime data, road/walkway networks, shuttle schedules, and campus infrastructure to suggest safe paths for University of Missouri students.

**This notebook covers:**
1. **Data Ingestion** — Loading and parsing all 9 data sources
2. **Exploratory Data Analysis** — Understanding distributions, patterns, and quality issues
3. **Data Visualization** — Crime heatmaps, temporal patterns, spatial clusters
4. **Feature Engineering** — Building safety-weighted edge attributes for the routing engine

**Data Sources:**
| # | Source | Format | Key Use |
|---|--------|--------|---------|
| 1 | OpenStreetMap (osmnx) | Graph/GeoDataFrame | Road & walkway network |
| 2 | CPD Dispatch CSV | CSV | Near-real-time crime incidents |
| 3 | MUPD Crime Data Hub | HTML/Links | University crime context |
| 4 | CPD Transparency Portal | ArcGIS REST/JSON | NIBRS-compliant geocoded crimes |
| 5 | MUPD Daily Crime Log | HTML Table | Campus-specific incidents |
| 6 | Clery Report (2025 PDF) | PDF | Aggregate Clery statistics |
| 7 | Tiger Line Shuttle (GTFS) | GTFS ZIP | Shuttle routes & stops |
| 8 | MU Campus Map | ArcGIS REST | Buildings, paths, infrastructure |
| 9 | MU Map Dashboard | ArcGIS REST | Construction zones, features |

---

## 1. Environment Setup & Imports

Install all dependencies first. The core stack is:
- **osmnx + networkx**: Street network download, graph routing
- **geopandas + shapely**: Spatial data manipulation
- **folium**: Interactive map visualization
- **pandas + numpy**: General data wrangling
- **matplotlib + seaborn**: Static plots
- **pdfplumber**: Clery report PDF extraction
- **scikit-learn**: KDE for crime density surfaces
- **requests + BeautifulSoup**: Web scraping for MUPD logs
"""

# ============================================================
# 1a. Install dependencies (run once)
# ============================================================
# !pip install osmnx geopandas folium matplotlib seaborn pandas numpy \
#     shapely scikit-learn pdfplumber requests beautifulsoup4 partridge \
#     contextily branca lxml --quiet

# ============================================================
# 1b. Imports
# ============================================================
import os
import json
import warnings
import re
from datetime import datetime, timedelta
from pathlib import Path

import numpy as np
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point, LineString, box
from shapely.ops import nearest_points

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns

# Geospatial
import osmnx as ox
import networkx as nx
import folium
from folium.plugins import HeatMap, HeatMapWithTime, MarkerCluster

# Scraping / Parsing
import requests
from bs4 import BeautifulSoup
import pdfplumber

# ML (for KDE)
from sklearn.neighbors import KernelDensity

# Config
warnings.filterwarnings("ignore")
sns.set_theme(style="whitegrid", palette="viridis")
plt.rcParams["figure.figsize"] = (14, 8)
plt.rcParams["figure.dpi"] = 100

# Project CRS constants
CRS_WGS84 = "EPSG:4326"       # Storage & display
CRS_UTM15N = "EPSG:32615"     # Distance calculations (meters, Columbia MO)

# Columbia, MO bounding box (approximate)
COLUMBIA_CENTER = (38.9517, -92.3341)
COLUMBIA_BBOX = {"north": 39.02, "south": 38.88, "east": -92.25, "west": -92.42}

print("All imports loaded successfully.")
print(f"osmnx version: {ox.__version__}")
print(f"geopandas version: {gpd.__version__}")
print(f"pandas version: {pd.__version__}")

"""---
## 2. Data Ingestion

Each source has a different format and access method. We load them one by one, inspect raw structure, and do minimal cleaning to get everything into GeoDataFrames.

### 2a. Source 1 — OSM Road & Walkway Network via osmnx

osmnx downloads OpenStreetMap data via the Overpass API and returns a routable NetworkX graph. We pull three network types (walk, bike, drive) and also fetch street lighting tags.
"""

# ============================================================
# 2a. Download OSM Networks for Columbia, MO
# ============================================================
CACHE_DIR = Path("./data/cache")
CACHE_DIR.mkdir(parents=True, exist_ok=True)

def load_or_download_graph(place, network_type, cache_dir=CACHE_DIR):
    """Download OSM graph or load from cache."""
    cache_file = cache_dir / f"graph_{network_type}.graphml"
    if cache_file.exists():
        print(f"  Loading cached {network_type} graph...")
        G = ox.load_graphml(cache_file)
    else:
        print(f"  Downloading {network_type} network for {place}...")
        G = ox.graph_from_place(place, network_type=network_type)
        ox.save_graphml(G, cache_file)
        print(f"  Saved to {cache_file}")
    return G

place = "Columbia, Missouri, USA"
print("Loading OSM networks:")
G_walk = load_or_download_graph(place, "walk")
G_bike = load_or_download_graph(place, "bike")
G_drive = load_or_download_graph(place, "drive")

for name, G in [("Walk", G_walk), ("Bike", G_bike), ("Drive", G_drive)]:
    print(f"  {name}: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges")

# ============================================================
# 2a (cont). Convert walk graph to GeoDataFrames
# ============================================================
nodes_gdf, edges_gdf = ox.graph_to_gdfs(G_walk, nodes=True, edges=True)

print("Walk Network Edges:")
print(f"  Shape: {edges_gdf.shape}")
print(f"  CRS: {edges_gdf.crs}")
print(f"  Columns: {list(edges_gdf.columns)}")
print(f"  Total network length: {edges_gdf['length'].sum() / 1000:.1f} km")
print(f"\nSample edge attributes:")
edges_gdf[["name", "highway", "length", "geometry"]].head(10)

# ============================================================
# 2a (cont). Fetch street lighting data from OSM
# ============================================================
try:
    lighting_gdf = ox.features_from_place(place, tags={"highway": "street_lamp"})
    print(f"Street lamps found: {len(lighting_gdf)}")
except Exception as e:
    print(f"No street_lamp features found: {e}")
    lighting_gdf = gpd.GeoDataFrame()

try:
    lit_features = ox.features_from_place(place, tags={"lit": True})
    print(f"Roads/paths with 'lit' tag: {len(lit_features)}")
except Exception as e:
    print(f"No lit-tagged features found: {e}")
    lit_features = gpd.GeoDataFrame()

# NOTE: OSM lighting coverage in Columbia is likely incomplete.
# Supplement with MU campus infrastructure (Source 8).

"""### 2b. Source 2 — Columbia Police Department Dispatch Data (CSV)

The CPD publishes a near-real-time dispatch feed with a direct CSV export endpoint. Data includes incident type, location (block-level addresses), and timestamp with a ~6-hour delay.

**Key fields:** InNum, DateTime, Location, InType (60+ categories)
"""

# ============================================================
# 2b. Load CPD Dispatch Data
# ============================================================
CPD_DISPATCH_URL = "https://www.como.gov/CMS/911dispatch/police_csvexport.php"

try:
    cpd_dispatch = pd.read_csv(CPD_DISPATCH_URL)
    print(f"CPD Dispatch records loaded: {len(cpd_dispatch):,}")
except Exception as e:
    print(f"Could not fetch live data: {e}")
    print("Creating sample structure for development...")
    cpd_dispatch = pd.DataFrame({
        "InNum":    ["CPD-2026-001234", "CPD-2026-001235", "CPD-2026-001236",
                     "CPD-2026-001237", "CPD-2026-001238", "CPD-2026-001239"],
        "DateTime": ["02/15/2026 01:23:00", "02/15/2026 02:45:00", "02/15/2026 03:10:00",
                     "02/14/2026 14:20:00", "02/14/2026 18:30:00", "02/14/2026 22:15:00"],
        "Location": ["1200 BLOCK ELLETA BLVD", "800 BLOCK BROADWAY",
                     "PROVIDENCE RD / STADIUM BLVD", "600 BLOCK TURNER AVE",
                     "1000 BLOCK UNIVERSITY AVE", "300 BLOCK S 9TH ST"],
        "InType":   ["ASSAULT 3RD", "STEALING", "SUSPICIOUS PERSON",
                     "PROPERTY DAMAGE", "ROBBERY 2ND", "DUI"]
    })

print(f"\nColumns: {list(cpd_dispatch.columns)}")
print(f"\nIncident type distribution (top 15):")
print(cpd_dispatch["InType"].value_counts().head(15))
cpd_dispatch.head()

# ============================================================
# 2b (cont). Parse datetimes and extract temporal features
# ============================================================
cpd_dispatch["datetime"] = pd.to_datetime(cpd_dispatch["DateTime"], errors="coerce")
cpd_dispatch["hour"] = cpd_dispatch["datetime"].dt.hour
cpd_dispatch["day_of_week"] = cpd_dispatch["datetime"].dt.day_name()
cpd_dispatch["month"] = cpd_dispatch["datetime"].dt.month
cpd_dispatch["date"] = cpd_dispatch["datetime"].dt.date

print(f"Date range: {cpd_dispatch['datetime'].min()} to {cpd_dispatch['datetime'].max()}")
print(f"Null datetimes: {cpd_dispatch['datetime'].isna().sum()}")

# ============================================================
# 2b (cont). Geocode CPD block-level addresses
# ============================================================
# IMPORTANT: Batch geocode BEFORE the hackathon and cache results!

def parse_cpd_address(raw_address):
    """Clean CPD dispatch addresses for geocoding.
    '1200 BLOCK ELLETA BLVD' -> '1200 ELLETA BLVD, Columbia, MO'
    'PROVIDENCE RD / STADIUM BLVD' -> 'PROVIDENCE RD & STADIUM BLVD, Columbia, MO'
    """
    if pd.isna(raw_address):
        return None
    addr = raw_address.strip().upper()
    addr = re.sub(r"\bBLOCK\b", "", addr).strip()
    addr = addr.replace(" / ", " & ")
    addr = re.sub(r"\s+", " ", addr)
    return f"{addr}, Columbia, MO"

cpd_dispatch["clean_address"] = cpd_dispatch["Location"].apply(parse_cpd_address)
print("Sample cleaned addresses:")
print(cpd_dispatch[["Location", "clean_address"]].drop_duplicates().head(10).to_string())

def geocode_address_nominatim(address, cache={}):
    """Geocode a single address via Nominatim with in-memory cache."""
    if address in cache:
        return cache[address]
    try:
        url = "https://nominatim.openstreetmap.org/search"
        params = {"q": address, "format": "json", "limit": 1,
                  "countrycodes": "us", "viewbox": "-92.42,39.02,-92.25,38.88"}
        resp = requests.get(url, params=params,
                            headers={"User-Agent": "TigerSafe-Hackathon/1.0"}, timeout=10)
        results = resp.json()
        if results:
            lat, lon = float(results[0]["lat"]), float(results[0]["lon"])
            cache[address] = (lat, lon)
            return (lat, lon)
    except Exception:
        pass
    cache[address] = (None, None)
    return (None, None)

# ---- Uncomment to run geocoding (respects rate limits) ----
# from time import sleep
# unique_addrs = cpd_dispatch["clean_address"].dropna().unique()
# print(f"Geocoding {len(unique_addrs)} unique addresses...")
# coords = []
# for i, addr in enumerate(unique_addrs):
#     lat, lon = geocode_address_nominatim(addr)
#     coords.append({"clean_address": addr, "lat": lat, "lon": lon})
#     if i % 50 == 0: print(f"  {i}/{len(unique_addrs)}")
#     sleep(1.1)
# pd.DataFrame(coords).to_csv("data/cache/cpd_geocoded.csv", index=False)

# ---- Load pre-geocoded results if available ----
geocode_cache = Path("data/cache/cpd_geocoded.csv")
if geocode_cache.exists():
    geocoded = pd.read_csv(geocode_cache)
    cpd_dispatch = cpd_dispatch.merge(geocoded, on="clean_address", how="left")
    print(f"Geocoding success rate: {cpd_dispatch['lat'].notna().mean()*100:.1f}%")
else:
    print("No geocoded cache found. Run geocoding step above first.")
    cpd_dispatch["lat"] = None
    cpd_dispatch["lon"] = None

"""### 2c. Source 4 — CPD Transparency Portal (ArcGIS REST API)

The CPD Transparency Portal is built on ArcGIS Hub. The underlying Feature Services can be queried via REST endpoints returning GeoJSON. This is likely the **highest-quality structured crime dataset** — NIBRS-compliant with geolocation.

**Discovery:** Open the portal in a browser, open DevTools > Network tab, capture FeatureServer URLs.
"""

# ============================================================
# 2c. Query CPD Transparency Portal ArcGIS REST API
# ============================================================
CPD_OPEN_DATA = "https://datahub-gocolumbiamo.opendata.arcgis.com"
CPD_ARCGIS_BASE = "https://gis.gocolumbiamo.com/arcgis/rest/services"

def query_arcgis_feature_layer(base_url, layer_id=0, where="1=1",
                                out_fields="*", out_sr=4326, max_records=2000):
    """Query an ArcGIS FeatureServer layer and return a GeoDataFrame."""
    url = f"{base_url}/{layer_id}/query"
    params = {
        "where": where, "outFields": out_fields, "outSR": out_sr,
        "f": "geojson", "resultRecordCount": max_records, "returnGeometry": "true"
    }
    try:
        resp = requests.get(url, params=params, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if "features" in data and len(data["features"]) > 0:
            gdf = gpd.GeoDataFrame.from_features(data["features"], crs=CRS_WGS84)
            print(f"  Loaded {len(gdf)} features from layer {layer_id}")
            return gdf
        else:
            print(f"  No features returned from layer {layer_id}")
            return None
    except Exception as e:
        print(f"  Error querying layer {layer_id}: {e}")
        return None

def discover_arcgis_services(base_url):
    """List all services on an ArcGIS REST server."""
    try:
        resp = requests.get(f"{base_url}?f=json", timeout=15)
        for svc in resp.json().get("services", []):
            print(f"  {svc['name']} ({svc['type']})")
    except Exception as e:
        print(f"Error: {e}")

# Example (replace with actual URL found via DevTools):
# cpd_crimes_gdf = query_arcgis_feature_layer(
#     "https://gis.gocolumbiamo.com/arcgis/rest/services/CPD_CrimeData/FeatureServer")
print("CPD Transparency Portal: Discover FeatureServer URLs via browser DevTools.")
print(f"Try: {CPD_ARCGIS_BASE}")
print(f"Or: {CPD_OPEN_DATA}")

"""### 2d. Source 5 — MUPD Daily Crime Log (HTML Table Scraping)

The MUPD crime log at `muop-mupdreports.missouri.edu/dclog.php` is a server-rendered HTML table with 7 columns: Case Number, Date/Time Reported, Location of Occurrence, Domestic DV Relationship, Incident Type, Criminal Offense, and Disposition.

Addresses are street-level (e.g., "820 CONLEY AVE, COLUMBIA MO, 65201") — more precise than CPD block-level data.
"""

# ============================================================
# 2d. Scrape MUPD Daily Crime Log
# ============================================================
MUPD_DCLOG_URL = "https://muop-mupdreports.missouri.edu/dclog.php"

def scrape_mupd_crime_log(url=MUPD_DCLOG_URL):
    """Scrape the MUPD daily crime log HTML table.

    Returns a cleaned DataFrame with parsed dates and locations.
    The page accepts time range parameters — default shows ~1 month.
    """
    try:
        tables = pd.read_html(url)
        if tables:
            df = tables[0]
            print(f"  Raw table shape: {df.shape}")
            print(f"  Columns: {list(df.columns)}")
            return df
        else:
            print("  No tables found on page.")
            return None
    except Exception as e:
        print(f"  Error scraping MUPD log: {e}")
        return None

mupd_log = scrape_mupd_crime_log()

if mupd_log is not None:
    print(f"\nMUPD Crime Log: {len(mupd_log)} records")
    mupd_log.head(10)
else:
    # Fallback: sample structure based on known schema
    print("Creating sample MUPD data structure...")
    mupd_log = pd.DataFrame({
        "Case Number": ["MUPD-2026-0100", "MUPD-2026-0101", "MUPD-2026-0102"],
        "Date/Time Reported": ["02/14/2026 22:15", "02/14/2026 23:30", "02/13/2026 15:45"],
        "Location of Occurrence": ["820 CONLEY AVE, COLUMBIA MO, 65201",
                                     "799 S COLLEGE AVE/ROLLINS ST",
                                     "1 UNIVERSITY BLVD, COLUMBIA MO"],
        "Domestic DV Relationship": ["No", "No", "No"],
        "Incident Type": ["Suspicious-Person", "Theft-From Auto", "Drug-Possession"],
        "Criminal Offense": ["Trespassing", "Stealing", "Controlled Substance"],
        "Disposition": ["Report Taken", "Under Investigation", "Arrest"]
    })
    mupd_log

import pandas as pd

# ============================================================
# 2d (cont). Clean and parse MUPD crime log
# ============================================================
def clean_mupd_log(df):
    """Parse MUPD crime log into analysis-ready format.

    Handles:
    - Date/time parsing for 'Reported' timestamps
    - Location parsing (street addresses vs intersections)
    - Deduplication by case number (multiple offenses per case)
    """
    df = df.copy()

    # Standardize column names
    # If columns are a MultiIndex (as from pd.read_html), flatten them to use the second level
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.map(lambda x: x[1].strip().lower().replace(" ", "_").replace("/", "_"))
    else:
        df.columns = [col.strip().lower().replace(" ", "_").replace("/", "_") for col in df.columns]

    # Parse datetime
    date_col = [c for c in df.columns if "date" in c and "reported" in c]
    if date_col:
        df["datetime_reported"] = pd.to_datetime(df[date_col[0]], errors="coerce")
        df["hour"] = df["datetime_reported"].dt.hour
        df["day_of_week"] = df["datetime_reported"].dt.day_name()

    # Parse location — detect intersection vs street address
    loc_col = [c for c in df.columns if "location" in c]
    if loc_col:
        df["raw_location"] = df[loc_col[0]]
        df["is_intersection"] = df["raw_location"].str.contains("/", na=False)

    # Count unique cases (some cases have multiple offense rows)
    case_col = [c for c in df.columns if "case" in c]
    if case_col:
        n_unique = df[case_col[0]].nunique()
        print(f"  Unique cases: {n_unique} (from {len(df)} rows)")

    return df

mupd_log = clean_mupd_log(mupd_log)
print(f"\nCleaned MUPD log columns: {list(mupd_log.columns)}")

"""### 2e. Source 6 — Clery Report (PDF Extraction)

The 2025 Annual Security & Fire Safety Report is a 201-page PDF. Crime statistics tables start around page 141. Clery data is **aggregate only** (On-Campus, Non-Campus, Public Property, Student Housing) — useful for context and trend narratives but not routing-grade.

An alternative structured source: the [DoE Campus Safety Database](https://ope.ed.gov/campussafety/) has the same data in queryable form.
"""

# ============================================================
# 2e. Extract Clery Crime Statistics from PDF
# ============================================================
CLERY_PDF_PATH = "data/2025-Annual-Security-and-Fire-Safety-Report.pdf"
CLERY_PDF_URL = ("https://mupolice.missouri.edu/wp-content/uploads/2025/09/"
                 "2025-Annual-Security-and-Fire-Safety-Report-Crime-Statistics-1.pdf")

def extract_clery_tables(pdf_path, start_page=140, end_page=178):
    """Extract crime statistics tables from the Clery report PDF.

    Uses pdfplumber for lattice-style table extraction.
    Returns a list of DataFrames, one per table found.
    """
    all_tables = []
    try:
        with pdfplumber.open(pdf_path) as pdf:
            print(f"  PDF has {len(pdf.pages)} pages")
            for i in range(start_page, min(end_page, len(pdf.pages))):
                page = pdf.pages[i]
                tables = page.extract_tables()
                for t in tables:
                    if len(t) > 1:
                        df = pd.DataFrame(t[1:], columns=t[0])
                        df["source_page"] = i + 1
                        all_tables.append(df)
            print(f"  Extracted {len(all_tables)} tables")
    except FileNotFoundError:
        print(f"  PDF not found at {pdf_path}.")
        print(f"  Download from: {CLERY_PDF_URL}")
    except Exception as e:
        print(f"  Error extracting tables: {e}")
    return all_tables

# Download PDF first, then uncomment:
# clery_tables = extract_clery_tables(CLERY_PDF_PATH)
# for i, t in enumerate(clery_tables[:3]):
#     print(f"\nTable {i} (page {t['source_page'].iloc[0]}):")
#     display(t.head())

print("Clery PDF extraction ready. Download PDF and uncomment above.")
print(f"Alternative: query https://ope.ed.gov/campussafety/ for structured data.")

"""### 2f. Source 7 — Tiger Line Shuttle (GTFS Feed)

The GoCOMO transit system (which includes Tiger Line) publishes a static GTFS feed. GTFS is a standardized format containing stops, routes, trips, stop times, and route shapes.
"""

# ============================================================
# 2f. Load and Parse GTFS Feed
# ============================================================
GTFS_URL = "https://s3.amazonaws.com/etatransit.gtfs/gocomotransit.etaspot.net/gtfs.zip"
GTFS_LOCAL = Path("data/cache/gtfs.zip")

def load_gtfs(gtfs_path):
    """Load GTFS feed and return dict of DataFrames.

    Standard GTFS files: routes, stops, stop_times, trips, shapes.
    """
    import zipfile
    extract_dir = Path(str(gtfs_path).replace(".zip", ""))
    if not extract_dir.exists():
        with zipfile.ZipFile(gtfs_path, "r") as z:
            z.extractall(extract_dir)
            print(f"  Extracted GTFS to {extract_dir}")

    gtfs = {}
    for fname in ["routes", "stops", "stop_times", "trips", "shapes"]:
        fpath = extract_dir / f"{fname}.txt"
        if fpath.exists():
            gtfs[fname] = pd.read_csv(fpath)
            print(f"  {fname}: {len(gtfs[fname])} records")
        else:
            print(f"  {fname}.txt not found")
    return gtfs

# Download GTFS feed
if not GTFS_LOCAL.exists():
    GTFS_LOCAL.parent.mkdir(parents=True, exist_ok=True)
    try:
        resp = requests.get(GTFS_URL, timeout=30)
        with open(GTFS_LOCAL, "wb") as f:
            f.write(resp.content)
        print(f"Downloaded GTFS feed ({len(resp.content)/1024:.0f} KB)")
    except Exception as e:
        print(f"Could not download GTFS: {e}")

if GTFS_LOCAL.exists():
    gtfs_data = load_gtfs(GTFS_LOCAL)

    # Filter for Tiger Line routes
    if "routes" in gtfs_data:
        tiger_pattern = r"Tiger|Hearnes|Trowbridge|Reactor|Campus Loop|Health Care"
        tiger_routes = gtfs_data["routes"][
            gtfs_data["routes"]["route_long_name"].str.contains(
                tiger_pattern, case=False, na=False)
        ]
        print(f"\nTiger Line routes found: {len(tiger_routes)}")
        if len(tiger_routes) > 0:
            display(tiger_routes)
else:
    print("GTFS feed not available. Download manually or check network.")

# ============================================================
# 2f (cont). Convert GTFS stops to GeoDataFrame
# ============================================================
stops_gdf = None
tiger_stops_gdf = None

if "gtfs_data" in dir() and "stops" in gtfs_data:
    stops_gdf = gpd.GeoDataFrame(
        gtfs_data["stops"],
        geometry=gpd.points_from_xy(
            gtfs_data["stops"]["stop_lon"],
            gtfs_data["stops"]["stop_lat"]
        ),
        crs=CRS_WGS84
    )
    print(f"Transit stops GeoDataFrame: {len(stops_gdf)} stops")

    # Filter to Tiger Line stops only
    if "tiger_routes" in dir() and len(tiger_routes) > 0:
        tiger_route_ids = tiger_routes["route_id"].tolist()
        tiger_trip_ids = gtfs_data["trips"][
            gtfs_data["trips"]["route_id"].isin(tiger_route_ids)
        ]["trip_id"].unique()
        tiger_stop_ids = gtfs_data["stop_times"][
            gtfs_data["stop_times"]["trip_id"].isin(tiger_trip_ids)
        ]["stop_id"].unique()
        tiger_stops_gdf = stops_gdf[stops_gdf["stop_id"].isin(tiger_stop_ids)]
        print(f"Tiger Line stops: {len(tiger_stops_gdf)}")
else:
    print("GTFS data not loaded. Skipping stop conversion.")

"""### 2g. Sources 8 & 9 — MU Campus Map & Dashboard (ArcGIS REST)

The MU campus map runs on ArcGIS at `map.missouri.edu:6443`. The base map provides building footprints and roads. The feature service includes queryable layers for construction zones, parking, and infrastructure.

**Note:** Native CRS is NAD 1983 StatePlane Missouri Central (WKID: 102697). Always request `outSR=4326` for WGS84.
"""

# ============================================================
# 2g. Query MU Campus ArcGIS Feature Layers
# ============================================================
MU_FEATURE_SERVER = ("https://map.missouri.edu:6443/arcgis/rest/services/"
                     "MU_Features_new/FeatureServer")
MU_MAP_SERVER = ("https://map.missouri.edu:6443/arcgis/rest/services/"
                 "MU_Base_new/MapServer")

def discover_arcgis_layers(service_url):
    """List all layers in an ArcGIS MapServer or FeatureServer."""
    try:
        resp = requests.get(f"{service_url}?f=json", timeout=15, verify=False)
        data = resp.json()
        layers = data.get("layers", [])
        print(f"Found {len(layers)} layers:")
        for layer in layers:
            geom = layer.get("geometryType", "N/A")
            print(f"  [{layer['id']}] {layer['name']} ({geom})")
        return layers
    except Exception as e:
        print(f"Error discovering layers: {e}")
        return []

# Discover available layers
print("=== MU Feature Layers ===")
feature_layers = discover_arcgis_layers(MU_FEATURE_SERVER)

print("\n=== MU Base Map Layers ===")
base_layers = discover_arcgis_layers(MU_MAP_SERVER)

# ============================================================
# 2g (cont). Query specific MU campus layers
# ============================================================
# After discovering layer IDs above, query relevant safety layers.
# Construction zones are confirmed as Layer 11.

def query_mu_layer(layer_id, layer_name="unknown"):
    """Query a specific MU campus feature layer."""
    gdf = query_arcgis_feature_layer(MU_FEATURE_SERVER, layer_id=layer_id)
    if gdf is not None:
        print(f"  {layer_name}: {len(gdf)} features")
        print(f"  Columns: {list(gdf.columns)[:8]}")
    return gdf

# Construction zones (confirmed as Layer 11)
print("Querying known layers:")
construction_gdf = query_mu_layer(11, "Construction Zones")

# Query other layers after discovery — uncomment and adjust IDs:
# buildings_gdf = query_mu_layer(0, "Buildings")
# sidewalks_gdf = query_mu_layer(3, "Sidewalks")
# emergency_phones_gdf = query_mu_layer(5, "Emergency Phones")
# parking_gdf = query_mu_layer(7, "Parking Lots")

"""---
## 3. Exploratory Data Analysis

Now that we have data loaded (or know how to load it), let's explore structure, quality, and patterns.

### 3a. Crime Data Quality Assessment
"""

# ============================================================
# 3a. Data quality report for all crime sources
# ============================================================
def data_quality_report(df, name):
    """Print a comprehensive data quality summary."""
    print(f"\n{'='*60}")
    print(f"  DATA QUALITY REPORT: {name}")
    print(f"{'='*60}")
    print(f"Shape: {df.shape[0]:,} rows x {df.shape[1]} columns")

    print(f"\nMissing values:")
    missing = df.isnull().sum()
    missing_pct = (missing / len(df) * 100).round(1)
    missing_report = pd.DataFrame({"missing": missing, "pct": missing_pct})
    nonzero = missing_report[missing_report["missing"] > 0]
    if len(nonzero) > 0:
        print(nonzero.to_string())
    else:
        print("  None!")

    print(f"\nDuplicates: {df.duplicated().sum()}")

    print(f"\nColumn types:")
    for dtype in df.dtypes.unique():
        cols = df.dtypes[df.dtypes == dtype].index.tolist()
        print(f"  {dtype}: {cols[:5]}{'...' if len(cols) > 5 else ''}")

data_quality_report(cpd_dispatch, "CPD Dispatch")
data_quality_report(mupd_log, "MUPD Daily Crime Log")

"""### 3b. Temporal Crime Patterns

Understanding **when** crimes occur is critical for the time-of-day safety multipliers in the routing engine. We analyze hourly, daily, and monthly distributions.
"""

# ============================================================
# 3b. Temporal analysis — hourly, daily, monthly crime patterns
# ============================================================
fig, axes = plt.subplots(1, 3, figsize=(20, 6))

# --- Hourly distribution ---
if "hour" in cpd_dispatch.columns and cpd_dispatch["hour"].notna().any():
    hourly = cpd_dispatch["hour"].value_counts().sort_index()
    axes[0].bar(hourly.index, hourly.values,
                color=sns.color_palette("viridis", len(hourly)),
                edgecolor="white", linewidth=0.5)
    axes[0].set_xlabel("Hour of Day")
    axes[0].set_ylabel("Incident Count")
    axes[0].set_title("CPD Incidents by Hour of Day")
    axes[0].set_xticks(range(0, 24, 2))
    # Add night shading
    axes[0].axvspan(0, 6, alpha=0.1, color="navy", label="Night (12am-6am)")
    axes[0].axvspan(18, 24, alpha=0.1, color="navy", label="Night (6pm-12am)")
    axes[0].legend(fontsize=9)
else:
    axes[0].text(0.5, 0.5, "No hourly data available",
                 ha="center", va="center", transform=axes[0].transAxes)

# --- Day of week ---
if "day_of_week" in cpd_dispatch.columns and cpd_dispatch["day_of_week"].notna().any():
    day_order = ["Monday", "Tuesday", "Wednesday", "Thursday",
                 "Friday", "Saturday", "Sunday"]
    daily = cpd_dispatch["day_of_week"].value_counts().reindex(day_order).fillna(0)
    colors = ["#e74c3c" if d in ["Friday", "Saturday"] else "#3498db" for d in day_order]
    axes[1].bar(range(7), daily.values, color=colors, edgecolor="white")
    axes[1].set_xticks(range(7))
    axes[1].set_xticklabels([d[:3] for d in day_order])
    axes[1].set_xlabel("Day of Week")
    axes[1].set_ylabel("Incident Count")
    axes[1].set_title("CPD Incidents by Day of Week\n(red = weekend)")

# --- Monthly trend ---
if "month" in cpd_dispatch.columns and cpd_dispatch["month"].notna().any():
    monthly = cpd_dispatch["month"].value_counts().sort_index()
    axes[2].plot(monthly.index, monthly.values, "o-", color="#2ecc71", linewidth=2)
    axes[2].fill_between(monthly.index, monthly.values, alpha=0.2, color="#2ecc71")
    axes[2].set_xlabel("Month")
    axes[2].set_ylabel("Incident Count")
    axes[2].set_title("CPD Incidents by Month")
    axes[2].set_xticks(range(1, 13))
    axes[2].set_xticklabels(["Jan","Feb","Mar","Apr","May","Jun",
                              "Jul","Aug","Sep","Oct","Nov","Dec"], fontsize=8)

plt.suptitle("Temporal Crime Patterns - Columbia, MO", fontsize=16, y=1.02)
plt.tight_layout()
os.makedirs("outputs", exist_ok=True)
plt.savefig("outputs/temporal_patterns.png", dpi=150, bbox_inches="tight")
plt.show()

# ============================================================
# 3b (cont). Calibrate time-of-day risk multipliers
# ============================================================
# These multipliers directly drive the routing engine's temporal weights.

time_periods = {
    "Late Night (12am-5am)": range(0, 5),
    "Early Morning (5am-8am)": range(5, 8),
    "Morning (8am-12pm)": range(8, 12),
    "Afternoon (12pm-5pm)": range(12, 17),
    "Evening (5pm-9pm)": range(17, 21),
    "Night (9pm-12am)": range(21, 24)
}

if "hour" in cpd_dispatch.columns and cpd_dispatch["hour"].notna().any():
    hourly_counts = cpd_dispatch["hour"].value_counts().sort_index()
    mean_hourly = hourly_counts.mean()

    print("Time Period Risk Multipliers (relative to average):")
    print("-" * 55)
    multipliers = {}
    for period, hours in time_periods.items():
        # Fix: Use fill_value=0 to treat hours with no incidents as 0, not NaN
        period_avg = hourly_counts.reindex(hours, fill_value=0).mean()
        multiplier = round(period_avg / mean_hourly, 2) if mean_hourly > 0 else 1.0
        multipliers[period] = multiplier
        bar = "#" * int(multiplier * 20)
        print(f"  {period:30s} {multiplier:.2f}x  {bar}")

    print(f"\n  Mean hourly incidents: {mean_hourly:.1f}")
    print(f"  Peak hour: {hourly_counts.idxmax()}:00 ({hourly_counts.max()} incidents)")
    print(f"  Lowest hour: {hourly_counts.idxmin()}:00 ({hourly_counts.min()} incidents)")
else:
    print("Insufficient temporal data. Using default multipliers from safety literature.")
    multipliers = {
        "Late Night (12am-5am)": 1.8,
        "Early Morning (5am-8am)": 0.5,
        "Morning (8am-12pm)": 0.5,
        "Afternoon (12pm-5pm)": 0.7,
        "Evening (5pm-9pm)": 1.0,
        "Night (9pm-12am)": 1.5,
    }

"""### 3c. Crime Type Distribution & Severity Analysis"""

# ============================================================
# 3c. Crime type analysis with violence categorization
# ============================================================
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

violent_keywords = ["ASSAULT", "ROBBERY", "SHOTS FIRED", "HOMICIDE",
                    "WEAPONS", "DOMESTIC", "STABBING", "RAPE", "SHOOTING"]

# --- Top crime types ---
if "InType" in cpd_dispatch.columns:
    top_types = cpd_dispatch["InType"].value_counts().head(20)
    colors = ["#e74c3c" if any(kw in str(t).upper() for kw in violent_keywords)
              else "#3498db" for t in top_types.index]

    axes[0].barh(range(len(top_types)), top_types.values, color=colors, edgecolor="white")
    axes[0].set_yticks(range(len(top_types)))
    axes[0].set_yticklabels(top_types.index, fontsize=9)
    axes[0].set_xlabel("Count")
    axes[0].set_title("Top 20 CPD Incident Types")
    axes[0].invert_yaxis()

    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor="#e74c3c", label="Violent"),
                       Patch(facecolor="#3498db", label="Non-Violent")]
    axes[0].legend(handles=legend_elements, loc="lower right")

# --- Violent vs non-violent share ---
if "InType" in cpd_dispatch.columns:
    cpd_dispatch["is_violent"] = cpd_dispatch["InType"].apply(
        lambda x: any(kw in str(x).upper() for kw in violent_keywords)
    )
    violence_share = cpd_dispatch["is_violent"].value_counts()
    axes[1].pie(violence_share.values,
                labels=["Non-Violent", "Violent"],
                colors=["#3498db", "#e74c3c"],
                autopct="%1.1f%%", startangle=90, textprops={"fontsize": 14})
    axes[1].set_title("Violent vs Non-Violent Incidents")

plt.suptitle("Crime Type Analysis - Columbia, MO", fontsize=16, y=1.02)
plt.tight_layout()
plt.savefig("outputs/crime_types.png", dpi=150, bbox_inches="tight")
plt.show()

"""### 3d. Network Analysis — Walk Network Structure"""

# ============================================================
# 3d. Walk network structure analysis
# ============================================================
fig, axes = plt.subplots(1, 3, figsize=(20, 6))

# --- Edge length distribution ---
axes[0].hist(edges_gdf["length"], bins=80, color="#2ecc71", edgecolor="white",
             alpha=0.8, range=(0, 500))
axes[0].set_xlabel("Edge Length (meters)")
axes[0].set_ylabel("Count")
axes[0].set_title("Walk Network Edge Lengths")
axes[0].axvline(edges_gdf["length"].median(), color="red", linestyle="--",
                label=f'Median: {edges_gdf["length"].median():.0f}m')
axes[0].legend()

# --- Highway type distribution ---
if "highway" in edges_gdf.columns:
    def get_first_highway(val):
        return val[0] if isinstance(val, list) else val
    hw_types = edges_gdf["highway"].apply(get_first_highway).value_counts().head(15)
    axes[1].barh(range(len(hw_types)), hw_types.values, color="#9b59b6", edgecolor="white")
    axes[1].set_yticks(range(len(hw_types)))
    axes[1].set_yticklabels(hw_types.index, fontsize=9)
    axes[1].set_xlabel("Edge Count")
    axes[1].set_title("Road Types in Walk Network")
    axes[1].invert_yaxis()

# --- Node degree distribution ---
degrees = [d for _, d in G_walk.degree()]
axes[2].hist(degrees, bins=range(1, max(degrees) + 2), color="#f39c12",
             edgecolor="white", alpha=0.8, align="left")
axes[2].set_xlabel("Node Degree (connections)")
axes[2].set_ylabel("Count")
axes[2].set_title("Walk Network Connectivity")
axes[2].axvline(np.mean(degrees), color="red", linestyle="--",
                label=f"Mean: {np.mean(degrees):.1f}")
axes[2].legend()

plt.suptitle("Walk Network Structure - Columbia, MO", fontsize=16, y=1.02)
plt.tight_layout()
plt.savefig("outputs/network_structure.png", dpi=150, bbox_inches="tight")
plt.show()

print(f"Walk network summary:")
print(f"  Nodes: {G_walk.number_of_nodes():,}")
print(f"  Edges: {G_walk.number_of_edges():,}")
print(f"  Total length: {edges_gdf['length'].sum()/1000:.1f} km")
print(f"  Avg edge length: {edges_gdf['length'].mean():.1f} m")
print(f"  Connected components: {nx.number_weakly_connected_components(G_walk)}")

"""---
## 4. Spatial Visualization

### 4a. Interactive Crime Heatmap with Folium
"""

# ============================================================
# 4a. Crime heatmap on interactive map
# ============================================================
def create_crime_heatmap(crime_df, lat_col="lat", lon_col="lon",
                         center=COLUMBIA_CENTER, zoom=13):
    """Create a Folium heatmap of crime incidents.

    Returns a folium.Map with a HeatMap layer using a
    blue-to-red gradient. Points without coordinates are skipped.
    """
    m = folium.Map(location=center, zoom_start=zoom, tiles="CartoDB dark_matter")

    valid = crime_df.dropna(subset=[lat_col, lon_col])
    if len(valid) == 0:
        print("No geocoded crime data available for heatmap.")
        folium.Marker(center, popup="No geocoded data loaded").add_to(m)
        return m

    heat_data = valid[[lat_col, lon_col]].values.tolist()
    HeatMap(
        heat_data, radius=18, blur=25,
        gradient={0.2: "blue", 0.4: "lime", 0.6: "yellow", 0.8: "orange", 1.0: "red"},
        min_opacity=0.3
    ).add_to(m)

    folium.LayerControl().add_to(m)
    print(f"Heatmap created with {len(heat_data)} points.")
    return m

crime_map = create_crime_heatmap(cpd_dispatch)
crime_map.save("outputs/crime_heatmap.html")
print("Saved: outputs/crime_heatmap.html")
crime_map

"""### 4b. Multi-Layer Safety Map

Overlays crime, transit stops, campus features, and the road network.
"""

# ============================================================
# 4b. Multi-layer safety map
# ============================================================
def create_safety_map(crime_df=None, transit_stops=None, construction=None,
                      lighting=None, center=COLUMBIA_CENTER, zoom=14):
    """Create a multi-layer interactive safety map with toggle controls.

    Layers: Crime heatmap, transit stops, construction zones, lighting.
    """
    m = folium.Map(location=center, zoom_start=zoom, tiles="CartoDB positron")

    # Crime heatmap
    if crime_df is not None:
        valid = crime_df.dropna(subset=["lat", "lon"])
        if len(valid) > 0:
            fg = folium.FeatureGroup(name="Crime Heatmap", show=True)
            HeatMap(valid[["lat", "lon"]].values.tolist(),
                    radius=15, blur=20, min_opacity=0.3).add_to(fg)
            fg.add_to(m)

    # Transit stops
    if transit_stops is not None and len(transit_stops) > 0:
        fg = folium.FeatureGroup(name="Transit Stops", show=True)
        for _, stop in transit_stops.iterrows():
            folium.CircleMarker(
                location=[stop.geometry.y, stop.geometry.x],
                radius=6, color="#3498db", fill=True, fill_opacity=0.8,
                popup=f"<b>{stop.get('stop_name', 'Stop')}</b>"
            ).add_to(fg)
        fg.add_to(m)

    # Construction zones
    if construction is not None and len(construction) > 0:
        fg = folium.FeatureGroup(name="Construction Zones", show=True)
        folium.GeoJson(
            construction.to_json(),
            style_function=lambda x: {
                "fillColor": "#e67e22", "color": "#d35400",
                "weight": 2, "fillOpacity": 0.4
            }
        ).add_to(fg)
        fg.add_to(m)

    # Street lighting
    if lighting is not None and len(lighting) > 0:
        fg = folium.FeatureGroup(name="Street Lighting", show=False)
        for _, lamp in lighting.iterrows():
            if lamp.geometry.geom_type == "Point":
                folium.CircleMarker(
                    location=[lamp.geometry.y, lamp.geometry.x],
                    radius=3, color="#f1c40f", fill=True, fill_opacity=0.7
                ).add_to(fg)
        fg.add_to(m)

    folium.LayerControl().add_to(m)
    return m

safety_map = create_safety_map(
    crime_df=cpd_dispatch,
    transit_stops=tiger_stops_gdf if tiger_stops_gdf is not None else stops_gdf,
    construction=construction_gdf if "construction_gdf" in dir() else None,
    lighting=lighting_gdf if len(lighting_gdf) > 0 else None
)
safety_map.save("outputs/safety_map_multilayer.html")
print("Saved: outputs/safety_map_multilayer.html")
safety_map

"""### 4c. Time-of-Day Crime Heatmap Animation

Shows how crime density shifts across hours — a compelling demo feature.
"""

# ============================================================
# 4c. Animated heatmap by hour of day
# ============================================================
def create_temporal_heatmap(crime_df, lat_col="lat", lon_col="lon",
                            center=COLUMBIA_CENTER, zoom=13):
    """Animated heatmap showing crime density by hour using HeatMapWithTime."""
    m = folium.Map(location=center, zoom_start=zoom, tiles="CartoDB dark_matter")

    valid = crime_df.dropna(subset=[lat_col, lon_col, "hour"])
    if len(valid) == 0:
        print("Insufficient geocoded data for temporal heatmap.")
        return m

    # Build data arrays for each hour
    heat_data_by_hour = []
    for hour in range(24):
        hour_data = valid[valid["hour"] == hour][[lat_col, lon_col]].values.tolist()
        heat_data_by_hour.append(hour_data)

    time_index = [f"{h:02d}:00" for h in range(24)]

    HeatMapWithTime(
        heat_data_by_hour, index=time_index,
        radius=20, auto_play=True, speed_step=0.5, max_opacity=0.8,
        gradient={0.2: "blue", 0.4: "lime", 0.6: "yellow", 0.8: "orange", 1.0: "red"}
    ).add_to(m)

    print(f"Temporal heatmap: {len(valid)} incidents across 24 hours")
    return m

temporal_map = create_temporal_heatmap(cpd_dispatch)
temporal_map.save("outputs/crime_heatmap_temporal.html")
print("Saved: outputs/crime_heatmap_temporal.html")
temporal_map

"""### 4d. Walk Network Visualization with osmnx"""

# ============================================================
# 4d. Static network comparison plot
# ============================================================
fig, axes = plt.subplots(1, 3, figsize=(24, 8))

ox.plot_graph(G_walk, ax=axes[0], node_size=0, edge_linewidth=0.3,
              edge_color="#2ecc71", bgcolor="white", show=False, close=False)
axes[0].set_title("Walk Network", fontsize=14)

ox.plot_graph(G_bike, ax=axes[1], node_size=0, edge_linewidth=0.3,
              edge_color="#3498db", bgcolor="white", show=False, close=False)
axes[1].set_title("Bike Network", fontsize=14)

ox.plot_graph(G_drive, ax=axes[2], node_size=0, edge_linewidth=0.3,
              edge_color="#e74c3c", bgcolor="white", show=False, close=False)
axes[2].set_title("Drive Network", fontsize=14)

plt.suptitle("OSM Network Types - Columbia, MO", fontsize=16, y=1.02)
plt.tight_layout()
plt.savefig("outputs/network_comparison.png", dpi=150, bbox_inches="tight")
plt.show()

"""---
## 5. Feature Engineering

This is where raw data becomes the **safety-weighted edge attributes** that power the routing engine.

Pipeline:
1. **Classify** all crime incidents into a unified taxonomy with severity weights
2. **Spatial join** crimes to nearest road segment (50m snap radius)
3. **Aggregate** crime counts, severity, and types per edge
4. **Compute** lighting scores from OSM + campus data
5. **Build** composite safety weights with temporal multipliers
6. **Export** the enriched graph for the routing engine

### 5a. Unified Crime Taxonomy
"""

# ============================================================
# 5a. Unified Crime Taxonomy
# ============================================================
# Severity weights are calibrated for PEDESTRIAN safety:
# a nearby robbery is far more dangerous to a walker than a property theft.

CRIME_TAXONOMY = {
    # ---- VIOLENT ----
    "homicide":           {"severity": 1.00, "violent": True,
                           "patterns": ["HOMICIDE", "MURDER", "MANSLAUGHTER"]},
    "sexual_assault":     {"severity": 0.95, "violent": True,
                           "patterns": ["RAPE", "SEX OFFENSE", "FONDLING", "SEXUAL"]},
    "robbery":            {"severity": 0.90, "violent": True,
                           "patterns": ["ROBBERY"]},
    "aggravated_assault": {"severity": 0.85, "violent": True,
                           "patterns": ["ASSAULT 1ST", "ASSAULT 2ND", "AGGRAVATED",
                                        "ASSAULT-AGGRAVATED"]},
    "simple_assault":     {"severity": 0.60, "violent": True,
                           "patterns": ["ASSAULT 3RD", "ASSAULT 4TH", "ASSAULT-SIMPLE",
                                        "DOMESTIC"]},
    "weapons":            {"severity": 0.70, "violent": True,
                           "patterns": ["SHOTS FIRED", "WEAPON", "ARMED", "SHOOTING",
                                        "STABBING"]},
    # ---- PROPERTY ----
    "burglary":           {"severity": 0.40, "violent": False,
                           "patterns": ["BURGLARY", "BREAKING"]},
    "theft":              {"severity": 0.30, "violent": False,
                           "patterns": ["STEALING", "THEFT", "LARCENY", "SHOPLIFTING"]},
    "vehicle_theft":      {"severity": 0.35, "violent": False,
                           "patterns": ["VEHICLE THEFT", "STOLEN VEHICLE", "THEFT-FROM AUTO"]},
    "vandalism":          {"severity": 0.20, "violent": False,
                           "patterns": ["PROPERTY DAMAGE", "VANDALISM"]},
    # ---- OTHER ----
    "drug_offense":       {"severity": 0.30, "violent": False,
                           "patterns": ["DRUG", "NARCOTIC", "CONTROLLED SUBSTANCE"]},
    "suspicious":         {"severity": 0.15, "violent": False,
                           "patterns": ["SUSPICIOUS", "TRESPASSING", "PROWLER"]},
    "disturbance":        {"severity": 0.25, "violent": False,
                           "patterns": ["DISTURBANCE", "PEACE", "NOISE", "FIGHT"]},
    "dui":                {"severity": 0.50, "violent": False,
                           "patterns": ["DUI", "DWI", "INTOXICATED DRIVER"]},
}

def classify_crime(raw_type, taxonomy=CRIME_TAXONOMY):
    """Map a raw crime type string to the unified taxonomy.

    Returns (category, severity, is_violent) tuple.
    Unmatched types default to ('other', 0.10, False).
    """
    if pd.isna(raw_type):
        return ("unknown", 0.10, False)
    raw_upper = str(raw_type).upper()
    for category, info in taxonomy.items():
        for pattern in info["patterns"]:
            if pattern in raw_upper:
                return (category, info["severity"], info["violent"])
    return ("other", 0.10, False)

# Apply taxonomy to CPD dispatch data
cpd_dispatch[["crime_category", "severity", "is_violent"]] = pd.DataFrame(
    cpd_dispatch["InType"].apply(classify_crime).tolist(),
    index=cpd_dispatch.index
)

print("Unified Crime Category Distribution:")
cat_counts = cpd_dispatch["crime_category"].value_counts()
for cat, count in cat_counts.items():
    sev = CRIME_TAXONOMY.get(cat, {}).get("severity", 0.1)
    print(f"  {cat:25s} {count:5d}  (severity: {sev})")
print(f"\nViolent incidents: {cpd_dispatch['is_violent'].sum()} "
      f"({cpd_dispatch['is_violent'].mean()*100:.1f}%)")

"""### 5b. Spatial Join — Snap Crimes to Road Edges"""

# ============================================================
# 5b. Spatial join — snap crimes to nearest road edge
# ============================================================
def spatial_join_crimes_to_edges(crimes_gdf, edges_gdf, max_distance_m=50,
                                 crs_projected=CRS_UTM15N):
    """Snap crime points to nearest road network edge.

    Parameters
    ----------
    max_distance_m : float
        Maximum snap distance in meters. Crimes farther are still
        assigned but flagged via within_snap_radius.

    Returns
    -------
    GeoDataFrame with nearest edge (u, v, key) and snap distance.
    """
    crimes_proj = crimes_gdf.to_crs(crs_projected)
    edges_reset = edges_gdf.to_crs(crs_projected).reset_index()

    joined = gpd.sjoin_nearest(
        crimes_proj, edges_reset,
        how="inner", max_distance=max_distance_m * 2, distance_col="snap_dist"
    )
    joined["within_snap_radius"] = joined["snap_dist"] <= max_distance_m

    print(f"  Crimes joined: {len(joined)}/{len(crimes_gdf)}")
    print(f"  Within {max_distance_m}m: {joined['within_snap_radius'].sum()}")
    print(f"  Median snap distance: {joined['snap_dist'].median():.1f}m")
    return joined

# Create crime GeoDataFrame (requires geocoded coordinates)
crime_edges = None
if cpd_dispatch["lat"].notna().any():
    crime_points = cpd_dispatch.dropna(subset=["lat", "lon"]).copy()
    crimes_gdf = gpd.GeoDataFrame(
        crime_points,
        geometry=gpd.points_from_xy(crime_points["lon"], crime_points["lat"]),
        crs=CRS_WGS84
    )
    print(f"Crime GeoDataFrame: {len(crimes_gdf)} geocoded incidents")
    crime_edges = spatial_join_crimes_to_edges(crimes_gdf, edges_gdf)
else:
    print("No geocoded crime data. Run geocoding (Section 2b) first.")

"""### 5c. Edge-Level Crime Aggregation"""

# ============================================================
# 5c. Aggregate crime statistics per road edge
# ============================================================
def aggregate_crimes_per_edge(crime_edges, edges_gdf):
    """Compute per-edge crime metrics for the routing engine.

    Metrics: crime_count, violent_count, crime_density (per 100m),
    max_severity, mean_severity, violent_ratio.
    """
    if crime_edges is None or len(crime_edges) == 0:
        print("No crime-edge data. Returning edges with zero crime counts.")
        return edges_gdf.assign(
            crime_count=0, violent_count=0, crime_density=0.0,
            max_severity=0.0, mean_severity=0.0, violent_ratio=0.0
        )

    agg = crime_edges.groupby(["u", "v", "key"]).agg(
        crime_count=("crime_category", "count"),
        violent_count=("is_violent", "sum"),
        max_severity=("severity", "max"),
        mean_severity=("severity", "mean"),
    ).reset_index()

    agg["violent_count"] = agg["violent_count"].astype(int)

    result = edges_gdf.reset_index().merge(
        agg, on=["u", "v", "key"], how="left"
    ).set_index(["u", "v", "key"])

    for col in ["crime_count", "violent_count", "max_severity", "mean_severity"]:
        result[col] = result[col].fillna(0)

    result["crime_density"] = np.where(
        result["length"] > 0,
        result["crime_count"] / (result["length"] / 100), 0
    )
    result["violent_ratio"] = np.where(
        result["crime_count"] > 0,
        result["violent_count"] / result["crime_count"], 0
    )

    edges_with_crime = (result["crime_count"] > 0).sum()
    print(f"\nEdge crime aggregation:")
    print(f"  Edges with >= 1 crime: {edges_with_crime}/{len(result)} "
          f"({edges_with_crime/len(result)*100:.1f}%)")
    print(f"  Max crimes on single edge: {result['crime_count'].max():.0f}")
    print(f"  Max crime density: {result['crime_density'].max():.2f} per 100m")
    return result

edges_enriched = aggregate_crimes_per_edge(crime_edges, edges_gdf)
edges_enriched[["length", "crime_count", "crime_density",
                "max_severity", "violent_ratio"]].describe().round(3)

"""### 5d. Lighting Score Computation"""

# ============================================================
# 5d. Compute lighting score per edge
# ============================================================
def compute_lighting_scores(edges_gdf, lighting_gdf=None, buffer_m=30):
    """Assign a lighting score (0-1) to each road edge.

    Combines:
    1. Road type defaults (primary > residential > footway)
    2. OSM lit=yes/no tag override
    3. Street lamp proximity (if lamp data available)
    """
    highway_defaults = {
        "primary": 0.7, "secondary": 0.6, "tertiary": 0.5,
        "residential": 0.4, "unclassified": 0.3,
        "footway": 0.2, "path": 0.15, "cycleway": 0.3,
        "service": 0.4, "living_street": 0.5,
    }

    # Component 1: Road type defaults
    if "highway" in edges_gdf.columns:
        def get_default(hw):
            if isinstance(hw, list): hw = hw[0]
            return highway_defaults.get(hw, 0.3)
        scores = edges_gdf["highway"].apply(get_default)
    else:
        scores = pd.Series(0.3, index=edges_gdf.index)

    # Component 2: OSM lit tag override
    if "lit" in edges_gdf.columns:
        lit_yes = edges_gdf["lit"].isin(["yes", True, "Yes"])
        lit_no = edges_gdf["lit"].isin(["no", False, "No"])
        scores[lit_yes] = 0.9
        scores[lit_no] = 0.1
        print(f"  Edges with lit=yes: {lit_yes.sum()}, lit=no: {lit_no.sum()}")

    scores.name = "lighting_score"
    print(f"  Lighting score range: [{scores.min():.2f}, {scores.max():.2f}]")
    print(f"  Mean lighting score: {scores.mean():.2f}")
    return scores

edges_enriched["lighting_score"] = compute_lighting_scores(edges_enriched, lighting_gdf)

# Visualize
fig, ax = plt.subplots(figsize=(10, 4))
edges_enriched["lighting_score"].hist(bins=30, ax=ax, color="#f1c40f",
                                       edgecolor="white", alpha=0.8)
ax.set_xlabel("Lighting Score")
ax.set_ylabel("Edge Count")
ax.set_title("Distribution of Lighting Scores Across Walk Network")
plt.tight_layout()
plt.savefig("outputs/lighting_distribution.png", dpi=150, bbox_inches="tight")
plt.show()

"""### 5e. Transit Proximity Feature"""

# ============================================================
# 5e. Transit proximity per edge
# ============================================================
def compute_transit_proximity(edges_gdf, stops_gdf, max_dist_m=200,
                               crs_projected=CRS_UTM15N):
    """Score each edge by proximity to transit stops.

    Edges near transit stops get a bonus — nearby transit
    provides a safer nighttime alternative to walking.
    """
    if stops_gdf is None or len(stops_gdf) == 0:
        print("  No transit stop data available.")
        return pd.DataFrame({
            "transit_dist_m": pd.Series(999.0, index=edges_gdf.index),
            "transit_score": pd.Series(0.0, index=edges_gdf.index)
        })

    edges_proj = edges_gdf.to_crs(crs_projected)
    stops_proj = stops_gdf.to_crs(crs_projected)

    # Midpoint of each edge -> nearest stop
    midpoints = edges_proj.geometry.interpolate(0.5, normalized=True)
    midpoints_gdf = gpd.GeoDataFrame(geometry=midpoints, crs=crs_projected)
    nearest = gpd.sjoin_nearest(midpoints_gdf, stops_proj, how="left",
                                 distance_col="transit_dist_m")

    # Score: 1.0 within 50m, decays to 0 at max_dist_m
    score = np.clip(1 - (nearest["transit_dist_m"] / max_dist_m), 0, 1)
    print(f"  Edges within {max_dist_m}m of transit: {(score > 0).sum()}")
    return pd.DataFrame({
        "transit_dist_m": nearest["transit_dist_m"].values,
        "transit_score": score.values
    }, index=edges_gdf.index)

transit_feat = compute_transit_proximity(
    edges_enriched,
    stops_gdf=tiger_stops_gdf if tiger_stops_gdf is not None else stops_gdf
)
edges_enriched["transit_dist_m"] = transit_feat["transit_dist_m"]
edges_enriched["transit_score"] = transit_feat["transit_score"]

"""### 5f. Composite Safety Weight — The Core Routing Feature

The final safety weight combines all features into a single edge cost for Dijkstra:

```
composite_weight = alpha * length_norm
                 + beta  * crime_density * temporal_multiplier
                 + gamma * (1 - lighting_score)
                 + delta * (1 - transit_score)
```

We precompute **day** and **night** weight columns. The routing engine selects the appropriate column based on query time.
"""

# ============================================================
# 5f. Compute composite safety weights
# ============================================================
def compute_safety_weights(edges_df, alpha=0.4, beta=0.4, gamma=0.15, delta=0.05):
    """Compute composite safety weight for routing.

    Returns weight_day, weight_night (for Dijkstra), and safety_score (for display).
    """
    df = edges_df.copy()

    # Normalize features to 0-1
    length_norm = df["length"] / df["length"].max()
    max_density = df["crime_density"].quantile(0.99)
    crime_norm = (df["crime_density"] / max_density).clip(0, 1) if max_density > 0 else 0
    crime_boosted = crime_norm * (1 + df["violent_ratio"])  # Violent crime multiplier
    light_penalty = 1 - df["lighting_score"].fillna(0.3)
    transit_penalty = 1 - df["transit_score"].fillna(0)

    # DAY: lower crime weight, lighting irrelevant
    df["weight_day"] = (
        alpha * length_norm
        + beta * crime_boosted * 0.5
        + gamma * light_penalty * 0.1
        + delta * transit_penalty
    )

    # NIGHT: full crime weight, lighting critical
    df["weight_night"] = (
        alpha * length_norm
        + beta * crime_boosted * 1.8
        + gamma * light_penalty * 1.0
        + delta * transit_penalty * 1.5
    )

    # SAFETY SCORE: 0 = dangerous, 1 = safe (for display/color coding)
    df["safety_score"] = 1 - (
        0.5 * crime_boosted.clip(0, 1)
        + 0.3 * light_penalty
        + 0.2 * transit_penalty
    ).clip(0, 1)

    print(f"Safety weight computation complete:")
    print(f"  Day weight   - mean: {df['weight_day'].mean():.4f}, std: {df['weight_day'].std():.4f}")
    print(f"  Night weight - mean: {df['weight_night'].mean():.4f}, std: {df['weight_night'].std():.4f}")
    print(f"  Safety score - mean: {df['safety_score'].mean():.3f}, "
          f"min: {df['safety_score'].min():.3f}, max: {df['safety_score'].max():.3f}")
    return df[["weight_day", "weight_night", "safety_score"]]

weights = compute_safety_weights(edges_enriched)
edges_enriched["weight_day"] = weights["weight_day"]
edges_enriched["weight_night"] = weights["weight_night"]
edges_enriched["safety_score"] = weights["safety_score"]

# Preview
print("\nFinal enriched edge columns:")
print(list(edges_enriched.columns))
edges_enriched[["length", "crime_count", "crime_density", "lighting_score",
                "transit_score", "weight_day", "weight_night", "safety_score"]].describe().round(4)

# ============================================================
# 5f (cont). Visualize safety scores on the network
# ============================================================
fig, axes = plt.subplots(1, 2, figsize=(20, 10))

if edges_enriched["safety_score"].std() > 0:
    # Safety score map
    edges_enriched.plot(
        column="safety_score", cmap="RdYlGn", linewidth=0.5,
        legend=True,
        legend_kwds={"label": "Safety Score (0=dangerous, 1=safe)", "shrink": 0.5},
        ax=axes[0]
    )
    axes[0].set_title("Walk Network - Safety Score", fontsize=14)
    axes[0].set_axis_off()

    # Night vs day weight difference
    edges_enriched["night_day_diff"] = (edges_enriched["weight_night"]
                                         - edges_enriched["weight_day"])
    edges_enriched.plot(
        column="night_day_diff", cmap="Reds", linewidth=0.5,
        legend=True,
        legend_kwds={"label": "Night-Day Weight Difference", "shrink": 0.5},
        ax=axes[1]
    )
    axes[1].set_title("Night vs Day Risk Increase", fontsize=14)
    axes[1].set_axis_off()
else:
    axes[0].text(0.5, 0.5, "Uniform safety scores (no crime data variation)",
                 ha="center", va="center", transform=axes[0].transAxes, fontsize=14)

plt.suptitle("Safety-Weighted Walk Network - Columbia, MO", fontsize=16)
plt.tight_layout()
plt.savefig("outputs/safety_network_map.png", dpi=150, bbox_inches="tight")
plt.show()

"""### 5g. Rebuild the Routing Graph with Safety Weights"""

# ============================================================
# 5g. Rebuild osmnx graph with safety weights
# ============================================================
def rebuild_graph_with_weights(G_original, edges_enriched):
    """Inject safety weights into the NetworkX graph for routing.

    Adds weight_day, weight_night, safety_score, crime_count, crime_density
    as edge attributes usable in shortest_path(weight=...).
    """
    G = G_original.copy()
    attrs = ["weight_day", "weight_night", "safety_score", "crime_count", "crime_density"]
    weight_lookup = edges_enriched[attrs].to_dict("index")

    updated = 0
    for (u, v, key), values in weight_lookup.items():
        if G.has_edge(u, v, key):
            for attr_name, attr_val in values.items():
                G[u][v][key][attr_name] = attr_val
            updated += 1

    print(f"Updated {updated}/{G.number_of_edges()} edges with safety weights")
    return G

G_safe = rebuild_graph_with_weights(G_walk, edges_enriched)

# Verify: sample one edge
sample = list(G_safe.edges(data=True, keys=True))[0]
print(f"\nSample edge ({sample[0]} -> {sample[1]}):")
for k, v in sample[3].items():
    print(f"  {k}: {v}")

# ============================================================
# 5g (cont). Demo: Compare shortest vs safest route
# ============================================================
def compare_routes(G, origin, dest, weight_col="weight_night"):
    """Compare shortest-distance route vs safest route.

    Parameters
    ----------
    G : networkx.MultiDiGraph
        Graph with safety weight attributes
    origin, dest : tuple
        (lat, lon) coordinates
    weight_col : str
        Edge attribute to minimize for 'safe' route
    """
    orig_node = ox.distance.nearest_nodes(G, origin[1], origin[0])
    dest_node = ox.distance.nearest_nodes(G, dest[1], dest[0])

    try:
        route_short = ox.routing.shortest_path(G, orig_node, dest_node, weight="length")
        route_safe = ox.routing.shortest_path(G, orig_node, dest_node, weight=weight_col)
    except nx.NetworkXNoPath:
        print("No path found between origin and destination.")
        return None

    def stats(route, name):
        edges = ox.routing.route_to_gdf(G, route)
        length = edges["length"].sum()
        safety = edges["safety_score"].mean() if "safety_score" in edges.columns else 0
        crimes = edges["crime_count"].sum() if "crime_count" in edges.columns else 0
        return {"route": route, "name": name, "length_m": length,
                "walk_min": length / 80, "safety": safety, "crimes": crimes}

    s = stats(route_short, "Shortest")
    f = stats(route_safe, "Safest")

    print(f"\n{'Route Comparison':=^50}")
    print(f"{'Metric':<25} {'Shortest':>12} {'Safest':>12}")
    print(f"{'-'*50}")
    print(f"{'Distance (m)':<25} {s['length_m']:>12.0f} {f['length_m']:>12.0f}")
    print(f"{'Walk time (min)':<25} {s['walk_min']:>12.1f} {f['walk_min']:>12.1f}")
    print(f"{'Avg safety score':<25} {s['safety']:>12.3f} {f['safety']:>12.3f}")
    print(f"{'Crimes on path':<25} {s['crimes']:>12.0f} {f['crimes']:>12.0f}")
    extra = f["length_m"] - s["length_m"]
    print(f"\nSafest route is {extra:.0f}m ({extra/80:.1f} min) longer")
    return {"shortest": s, "safest": f}

# Demo: MU Rec Center -> Downtown Broadway
ORIGIN = (38.9407, -92.3288)   # MU Student Rec Center
DEST   = (38.9517, -92.3341)   # Downtown Broadway

print("Demo: MU Rec Center -> Downtown Broadway (nighttime)")
comparison = compare_routes(G_safe, ORIGIN, DEST, weight_col="weight_night")

# ============================================================
# 5g (cont). Visualize route comparison
# ============================================================
if comparison is not None:
    fig, ax = ox.plot_graph(G_walk, node_size=0, edge_linewidth=0.2,
                            edge_color="#cccccc", bgcolor="white",
                            show=False, close=False, figsize=(14, 14))

    # Shortest (red) and Safest (green)
    ox.plot_graph_route(G_walk, comparison["shortest"]["route"],
                        route_color="#e74c3c", route_linewidth=4,
                        route_alpha=0.7, ax=ax, show=False, close=False)
    ox.plot_graph_route(G_walk, comparison["safest"]["route"],
                        route_color="#2ecc71", route_linewidth=4,
                        route_alpha=0.7, ax=ax, show=False, close=False)

    ax.scatter(*reversed(ORIGIN), c="blue", s=200, zorder=5, marker="^")
    ax.scatter(*reversed(DEST), c="purple", s=200, zorder=5, marker="s")
    ax.legend(["Shortest (red)", "Safest (green)", "Origin", "Destination"],
              fontsize=12, loc="upper right")
    ax.set_title("Shortest vs Safest Route - Nighttime", fontsize=16)

    plt.savefig("outputs/route_comparison.png", dpi=150, bbox_inches="tight")
    plt.show()

"""---
## 6. Export Enriched Data for the App

Save the enriched graph and crime database for the Streamlit app and Claude agent.
"""

# ============================================================
# 6. Export all processed data
# ============================================================
OUTPUT_DIR = Path("data/processed")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Enriched walk graph
ox.save_graphml(G_safe, OUTPUT_DIR / "columbia_walk_safety.graphml")
print(f"Saved: {OUTPUT_DIR / 'columbia_walk_safety.graphml'}")

# GeoPackage with all layers
edges_enriched.to_file(OUTPUT_DIR / "safety_layers.gpkg", layer="edges", driver="GPKG")
nodes_gdf.to_file(OUTPUT_DIR / "safety_layers.gpkg", layer="nodes", driver="GPKG")
if "crimes_gdf" in dir() and crimes_gdf is not None:
    crimes_gdf.to_file(OUTPUT_DIR / "safety_layers.gpkg", layer="crimes", driver="GPKG")
print(f"Saved: {OUTPUT_DIR / 'safety_layers.gpkg'}")

# Crime CSV for Claude agent tools
if "crimes_gdf" in dir() and crimes_gdf is not None:
    crimes_gdf.drop(columns=["geometry"]).to_csv(
        OUTPUT_DIR / "crimes_geocoded.csv", index=False)
    print(f"Saved: {OUTPUT_DIR / 'crimes_geocoded.csv'}")

# Routing config JSON
config = {
    "temporal_multipliers": multipliers,
    "weight_coefficients": {"alpha": 0.4, "beta": 0.4, "gamma": 0.15, "delta": 0.05},
    "generated_at": datetime.now().isoformat(),
}
with open(OUTPUT_DIR / "routing_config.json", "w") as f:
    json.dump(config, f, indent=2)
print(f"Saved: {OUTPUT_DIR / 'routing_config.json'}")

print(f"\nAll outputs saved to {OUTPUT_DIR.resolve()}")
print("These files are ready for the Streamlit app and Claude agent.")

"""---
## Next Steps

With this notebook complete, you have:

1. **Data ingestion pipelines** for all 9 sources (OSM, CPD, MUPD, GTFS, ArcGIS, Clery)
2. **A unified crime taxonomy** mapping CPD/MUPD/Clery categories to severity-weighted labels
3. **Temporal risk multipliers** calibrated from actual dispatch data
4. **An enriched walk graph** with per-edge crime density, lighting score, transit proximity, and composite safety weights
5. **Exported artifacts** ready for the Streamlit app and Claude tool-use agent

### Pre-Hackathon Checklist
- [ ] Run geocoding on all CPD dispatch addresses (Section 2b)
- [ ] Discover and query CPD Transparency Portal ArcGIS endpoints (Section 2c)
- [ ] Download Clery PDF and extract tables (Section 2e)
- [ ] Download GTFS feed and identify Tiger Line routes (Section 2f)
- [ ] Discover MU campus ArcGIS layer IDs (Section 2g)
- [ ] Run full notebook end-to-end and cache all outputs

### For the Hackathon
- Build the **Streamlit app** with `streamlit-folium` for the map + chat UI
- Implement the **Claude tool-use agent** with 5 geospatial tools
- Wire up the **route comparison** (shortest vs safest) with map visualization
- Add the **temporal heatmap animation** as a demo-ready feature
"""